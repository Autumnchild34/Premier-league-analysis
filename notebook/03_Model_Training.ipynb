{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bc8a96-04a6-4f28-bcd3-67bf3e41bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned data\n",
    "stats = pd.read_csv('cleaned_stats.csv')\n",
    "results = pd.read_csv('cleaned_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01530fc1-1b83-4811-b576-82a6b12c503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Problem Formulation & Target Creation \n",
    "\n",
    "# We need to compute final standings from results for each season\n",
    "# Create a table of points per team per season from results\n",
    "def compute_season_points(df):\n",
    "    # Home points\n",
    "    home = df.groupby(['season', 'home_team']).agg(\n",
    "        home_wins=('result', lambda x: (x=='H').sum()),\n",
    "        home_draws=('result', lambda x: (x=='D').sum()),\n",
    "        home_goals_for=('home_goals', 'sum'),\n",
    "        home_goals_against=('away_goals', 'sum')\n",
    "    ).reset_index().rename(columns={'home_team': 'team'})\n",
    "\n",
    "    # Away points\n",
    "    away = df.groupby(['season', 'away_team']).agg(\n",
    "        away_wins=('result', lambda x: (x=='A').sum()),\n",
    "        away_draws=('result', lambda x: (x=='D').sum()),\n",
    "        away_goals_for=('away_goals', 'sum'),\n",
    "        away_goals_against=('home_goals', 'sum')\n",
    "    ).reset_index().rename(columns={'away_team': 'team'})\n",
    "\n",
    "    # Merge and compute totals\n",
    "    standings = home.merge(away, on=['season', 'team'], how='outer').fillna(0)\n",
    "    standings['wins'] = standings['home_wins'] + standings['away_wins']\n",
    "    standings['draws'] = standings['home_draws'] + standings['away_draws']\n",
    "    standings['losses'] = 38 - standings['wins'] - standings['draws']\n",
    "    standings['points'] = 3*standings['wins'] + standings['draws']\n",
    "    standings['goals_for'] = standings['home_goals_for'] + standings['away_goals_for']\n",
    "    standings['goals_against'] = standings['home_goals_against'] + standings['away_goals_against']\n",
    "    standings['goal_diff'] = standings['goals_for'] - standings['goals_against']\n",
    "\n",
    "    # Rank within season\n",
    "    standings = standings.sort_values(['season', 'points', 'goal_diff', 'goals_for'],\n",
    "                                       ascending=[True, False, False, False])\n",
    "    standings['rank'] = standings.groupby('season').cumcount() + 1\n",
    "    return standings\n",
    "\n",
    "standings = compute_season_points(results)\n",
    "\n",
    "# Merge with stats to get features and targets together\n",
    "# stats already contains wins, draws, losses, goals, goals_conceded, etc.\n",
    "# We'll use stats for features and also add targets from standings where needed.\n",
    "# But note: stats has aggregated numbers that match the season totals (should be consistent)\n",
    "# We'll join on team and season\n",
    "stats['season'] = stats['season'].astype(str)\n",
    "standings['season'] = standings['season'].astype(str)\n",
    "\n",
    "df = stats.merge(standings[['season', 'team', 'rank', 'points', 'goals_for', 'goals_against',\n",
    "                            'wins', 'draws', 'losses']],\n",
    "                 on=['season', 'team'], how='left', suffixes=('', '_target'))\n",
    "\n",
    "# Verify consistency (optional)\n",
    "# assert (df['wins'] == df['wins_target']).all()  # might fail if stats are from a different source; we'll trust stats.\n",
    "\n",
    "# Create binary targets\n",
    "df['top4'] = (df['rank'] <= 4).astype(int)\n",
    "df['relegated'] = (df['rank'] >= 18).astype(int)  # 20 teams, bottom 3\n",
    "\n",
    "# Also create targets for the regression tasks (already present in stats or we can use stats)\n",
    "# Goals scored, goals conceded, clean sheets, shots on target, big chances missed, total passes, touches\n",
    "# But note: some targets like big chances missed are only in stats (big_chance_missed)\n",
    "# We'll keep stats columns as targets directly for those tasks.\n",
    "# However, we must ensure we don't use the same stats as features for the same season (data leakage).\n",
    "# We will use only features from *previous* seasons. That requires creating lagged features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21df7bfc-fd8f-4afb-9c13-678c018f5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Feature Engineering (Lagged Features) \n",
    "\n",
    "\n",
    "\n",
    "# Sort by team and season\n",
    "df = df.sort_values(['team', 'season_start'])\n",
    "\n",
    "# Create lag features for each numeric stat (excluding targets we want to predict)\n",
    "feature_cols = [c for c in stats.columns if c not in\n",
    "                ['team', 'season', 'season_start', 'wins', 'losses', 'draws',\n",
    "                 'points', 'goal_diff', 'total_matches', 'strength', 'cluster']]\n",
    "# Also exclude the targets we'll predict separately (to avoid leakage)\n",
    "# We'll create lags for all numeric features.\n",
    "\n",
    "# For each team, shift numeric features by 1 season\n",
    "lagged = df.groupby('team')[feature_cols].shift(1).add_suffix('_lag1')\n",
    "df = pd.concat([df, lagged], axis=1)\n",
    "\n",
    "# Also create 2-season moving averages for stability\n",
    "for col in feature_cols:\n",
    "    df[col + '_ma2'] = df.groupby('team')[col].transform(lambda x: x.rolling(2, min_periods=1).mean().shift(1))\n",
    "\n",
    "# Drop rows where no lag features exist (first season for each team)\n",
    "df = df.dropna(subset=df.filter(like='_lag1').columns, how='all').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ae93af-20bc-40c6-9345-965cde602704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (144, 138), Valid: (19, 138), Test: (38, 138)\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Define Training and Test Sets (Temporal Split)\n",
    "\n",
    "\n",
    "# Use seasons up to 2014 for training, 2015 for validation, 2016-2017 for test\n",
    "train = df[df['season_start'] <= 2014]\n",
    "valid = df[df['season_start'] == 2015]\n",
    "test = df[df['season_start'] >= 2016]\n",
    "\n",
    "print(f\"Train: {train.shape}, Valid: {valid.shape}, Test: {test.shape}\")\n",
    "\n",
    "# Features: all lag columns + any static team info? We can one-hot encode team.\n",
    "# But team identity may be important; we'll include as categorical feature.\n",
    "# We'll use Label Encoding for tree models, or one-hot for linear.\n",
    "\n",
    "# Prepare feature matrix (X) and target vectors for each task\n",
    "# We'll define a function to train/evaluate each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53cd6912-9507-4df1-9f0c-1298768fedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Helper Functions for Modeling  \n",
    "\n",
    "def get_X_y(df, target_col, classification=True):\n",
    "    \"\"\"Return features and target for given target column.\"\"\"\n",
    "    # Features: all lag1 and ma2 columns, plus team encoded\n",
    "    feature_pattern = ['_lag1$', '_ma2$']  # columns ending with these\n",
    "    feat_cols = [c for c in df.columns if any(p in c for p in feature_pattern)]\n",
    "    # Also add team as categorical (we'll encode later)\n",
    "    X = df[feat_cols].copy()\n",
    "    # Add team label encoding (simple)\n",
    "    le = LabelEncoder()\n",
    "    X['team_encoded'] = le.fit_transform(df['team'])\n",
    "    y = df[target_col].copy()\n",
    "    if classification and y.dtype not in ['int64', 'int32']:\n",
    "        y = y.astype(int)\n",
    "    return X, y, feat_cols\n",
    "\n",
    "def evaluate_regression(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, y_prob, model_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_true, y_prob[:, 1]) if y_prob is not None else np.nan\n",
    "    print(f\"{model_name} - Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n",
    "    return {'Accuracy': acc, 'F1': f1, 'AUC': auc}\n",
    "\n",
    "# Define models\n",
    "reg_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "clf_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38edfa4f-5225-41d8-803c-df8f1c93b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target: top4 ---\n",
      "Logistic Regression - Acc: 0.789, F1: 0.697, AUC: 0.650\n",
      "Random Forest - Acc: 0.684, F1: 0.684, AUC: 0.683\n",
      "XGBoost - Acc: 0.684, F1: 0.684, AUC: 0.542\n",
      "LightGBM - Acc: 0.737, F1: 0.747, AUC: 0.583\n",
      "\n",
      "--- Target: relegated ---\n",
      "Logistic Regression - Acc: 0.842, F1: 0.770, AUC: 0.354\n",
      "Random Forest - Acc: 0.789, F1: 0.789, AUC: 0.510\n",
      "XGBoost - Acc: 0.789, F1: 0.789, AUC: 0.573\n",
      "LightGBM - Acc: 0.789, F1: 0.743, AUC: 0.615\n",
      "\n",
      "--- Target: goals_scored ---\n",
      "Linear Regression - MAE: 11.37, RMSE: 13.42, R2: -0.059\n",
      "Ridge - MAE: 11.37, RMSE: 13.41, R2: -0.058\n",
      "Random Forest - MAE: 14.09, RMSE: 17.60, R2: -0.821\n",
      "XGBoost - MAE: 15.41, RMSE: 19.00, R2: -1.123\n",
      "LightGBM - MAE: 11.74, RMSE: 15.59, R2: -0.430\n",
      "\n",
      "--- Target: goals_conceded ---\n",
      "Linear Regression - MAE: 9.34, RMSE: 11.81, R2: -0.103\n",
      "Ridge - MAE: 9.33, RMSE: 11.80, R2: -0.102\n",
      "Random Forest - MAE: 10.94, RMSE: 13.68, R2: -0.480\n",
      "XGBoost - MAE: 11.35, RMSE: 13.74, R2: -0.492\n",
      "LightGBM - MAE: 10.25, RMSE: 12.96, R2: -0.328\n",
      "\n",
      "--- Target: wins ---\n",
      "Linear Regression - MAE: 4.44, RMSE: 5.34, R2: -0.187\n",
      "Ridge - MAE: 4.44, RMSE: 5.34, R2: -0.185\n",
      "Random Forest - MAE: 4.73, RMSE: 6.66, R2: -0.843\n",
      "XGBoost - MAE: 5.19, RMSE: 7.04, R2: -1.060\n",
      "LightGBM - MAE: 4.57, RMSE: 5.88, R2: -0.437\n",
      "\n",
      "--- Target: draws ---\n",
      "Linear Regression - MAE: 1.98, RMSE: 2.37, R2: -0.227\n",
      "Ridge - MAE: 1.98, RMSE: 2.37, R2: -0.226\n",
      "Random Forest - MAE: 2.50, RMSE: 3.00, R2: -0.961\n",
      "XGBoost - MAE: 2.40, RMSE: 2.95, R2: -0.892\n",
      "LightGBM - MAE: 2.50, RMSE: 2.88, R2: -0.805\n",
      "\n",
      "--- Target: losses ---\n",
      "Linear Regression - MAE: 4.54, RMSE: 5.93, R2: -0.076\n",
      "Ridge - MAE: 4.54, RMSE: 5.93, R2: -0.075\n",
      "Random Forest - MAE: 5.22, RMSE: 7.19, R2: -0.582\n",
      "XGBoost - MAE: 5.51, RMSE: 7.37, R2: -0.661\n",
      "LightGBM - MAE: 4.85, RMSE: 6.41, R2: -0.256\n",
      "\n",
      "--- Target: clean_sheets ---\n",
      "Linear Regression - MAE: 3.01, RMSE: 3.69, R2: -0.040\n",
      "Ridge - MAE: 3.00, RMSE: 3.69, R2: -0.039\n",
      "Random Forest - MAE: 3.57, RMSE: 4.32, R2: -0.426\n",
      "XGBoost - MAE: 3.80, RMSE: 4.60, R2: -0.613\n",
      "LightGBM - MAE: 3.51, RMSE: 4.21, R2: -0.353\n",
      "\n",
      "--- Target: shots_on_target ---\n",
      "Linear Regression - MAE: 33.78, RMSE: 39.83, R2: -0.176\n",
      "Ridge - MAE: 33.77, RMSE: 39.81, R2: -0.176\n",
      "Random Forest - MAE: 34.82, RMSE: 43.64, R2: -0.412\n",
      "XGBoost - MAE: 36.91, RMSE: 46.41, R2: -0.597\n",
      "LightGBM - MAE: 35.74, RMSE: 44.72, R2: -0.483\n",
      "\n",
      "--- Target: big_chances_missed ---\n",
      "Linear Regression - MAE: 10.54, RMSE: 12.31, R2: -0.022\n",
      "Ridge - MAE: 10.54, RMSE: 12.31, R2: -0.022\n",
      "Random Forest - MAE: 10.59, RMSE: 12.94, R2: -0.130\n",
      "XGBoost - MAE: 11.10, RMSE: 13.94, R2: -0.311\n",
      "LightGBM - MAE: 11.41, RMSE: 13.00, R2: -0.141\n",
      "\n",
      "--- Target: total_passes ---\n",
      "Linear Regression - MAE: 2333.19, RMSE: 2710.60, R2: 0.028\n",
      "Ridge - MAE: 2333.72, RMSE: 2711.60, R2: 0.028\n",
      "Random Forest - MAE: 2709.33, RMSE: 3463.42, R2: -0.586\n",
      "XGBoost - MAE: 2911.51, RMSE: 3794.55, R2: -0.904\n",
      "LightGBM - MAE: 2919.16, RMSE: 3417.93, R2: -0.545\n",
      "\n",
      "--- Target: touches ---\n",
      "Linear Regression - MAE: 2434.38, RMSE: 2683.96, R2: 0.086\n",
      "Ridge - MAE: 2434.84, RMSE: 2684.53, R2: 0.085\n",
      "Random Forest - MAE: 2577.99, RMSE: 3317.42, R2: -0.397\n",
      "XGBoost - MAE: 2765.57, RMSE: 3621.42, R2: -0.665\n",
      "LightGBM - MAE: 2910.37, RMSE: 3289.50, R2: -0.373\n",
      "\n",
      "Summary of best models on validation set:\n",
      "top4: {'best_model': 'Random Forest', 'val_auc': np.float64(0.6833333333333333)}\n",
      "relegated: {'best_model': 'LightGBM', 'val_auc': np.float64(0.6145833333333333)}\n",
      "goals_scored: {'best_model': 'Ridge', 'val_rmse': np.float64(13.41239124541271)}\n",
      "goals_conceded: {'best_model': 'Ridge', 'val_rmse': np.float64(11.803767569431928)}\n",
      "wins: {'best_model': 'Ridge', 'val_rmse': np.float64(5.340214412716076)}\n",
      "draws: {'best_model': 'Ridge', 'val_rmse': np.float64(2.371975755212418)}\n",
      "losses: {'best_model': 'Ridge', 'val_rmse': np.float64(5.925042382848866)}\n",
      "clean_sheets: {'best_model': 'Ridge', 'val_rmse': np.float64(3.6900918152497146)}\n",
      "shots_on_target: {'best_model': 'Ridge', 'val_rmse': np.float64(39.81306331535796)}\n",
      "big_chances_missed: {'best_model': 'Ridge', 'val_rmse': np.float64(12.305680048407552)}\n",
      "total_passes: {'best_model': 'Linear Regression', 'val_rmse': np.float64(2710.5997638999215)}\n",
      "touches: {'best_model': 'Linear Regression', 'val_rmse': np.float64(2683.960858726572)}\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Train Models for Each Target\n",
    "\n",
    "\n",
    "# Define all targets with their type and column name in df\n",
    "targets = [\n",
    "    {'name': 'top4', 'type': 'classification', 'col': 'top4'},\n",
    "    {'name': 'relegated', 'type': 'classification', 'col': 'relegated'},\n",
    "    {'name': 'goals_scored', 'type': 'regression', 'col': 'goals'},\n",
    "    {'name': 'goals_conceded', 'type': 'regression', 'col': 'goals_conceded'},\n",
    "    {'name': 'wins', 'type': 'regression', 'col': 'wins'},          # but wins are already in stats; we'll predict wins from lagged features\n",
    "    {'name': 'draws', 'type': 'regression', 'col': 'draws'},\n",
    "    {'name': 'losses', 'type': 'regression', 'col': 'losses'},\n",
    "    {'name': 'clean_sheets', 'type': 'regression', 'col': 'clean_sheet'},\n",
    "    {'name': 'shots_on_target', 'type': 'regression', 'col': 'ontarget_scoring_att'},\n",
    "    {'name': 'big_chances_missed', 'type': 'regression', 'col': 'big_chance_missed'},\n",
    "    {'name': 'total_passes', 'type': 'regression', 'col': 'total_pass'},\n",
    "    {'name': 'touches', 'type': 'regression', 'col': 'touches'}\n",
    "]\n",
    "\n",
    "# For multi-output (wins, draws, losses) we'll treat as separate regressions,\n",
    "# but could also use multi-output regressor. We'll keep separate for simplicity.\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n--- Target: {target['name']} ---\")\n",
    "    X_train, y_train, feat_list = get_X_y(train, target['col'], classification=(target['type']=='classification'))\n",
    "    X_val, y_val, _ = get_X_y(valid, target['col'], classification=(target['type']=='classification'))\n",
    "\n",
    "    # Standardize features for linear models only (trees don't need scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    if target['type'] == 'classification':\n",
    "        models = clf_models\n",
    "        best_model = None\n",
    "        best_auc = 0\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled if name in ['Logistic Regression'] else X_train, y_train)\n",
    "            y_pred = model.predict(X_val_scaled if name in ['Logistic Regression'] else X_val)\n",
    "            y_prob = model.predict_proba(X_val_scaled if name in ['Logistic Regression'] else X_val) if hasattr(model, 'predict_proba') else None\n",
    "            metrics = evaluate_classification(y_val, y_pred, y_prob, name)\n",
    "            if metrics['AUC'] > best_auc:\n",
    "                best_auc = metrics['AUC']\n",
    "                best_model = name\n",
    "        results_summary[target['name']] = {'best_model': best_model, 'val_auc': best_auc}\n",
    "    else:\n",
    "        models = reg_models\n",
    "        best_model = None\n",
    "        best_rmse = np.inf\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled if name in ['Linear Regression', 'Ridge'] else X_train, y_train)\n",
    "            y_pred = model.predict(X_val_scaled if name in ['Linear Regression', 'Ridge'] else X_val)\n",
    "            metrics = evaluate_regression(y_val, y_pred, name)\n",
    "            if metrics['RMSE'] < best_rmse:\n",
    "                best_rmse = metrics['RMSE']\n",
    "                best_model = name\n",
    "        results_summary[target['name']] = {'best_model': best_model, 'val_rmse': best_rmse}\n",
    "\n",
    "print(\"\\nSummary of best models on validation set:\")\n",
    "for k, v in results_summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# We'll save the best models and scalers for later evaluation on test set.\n",
    "# For simplicity, we'll retrain on combined train+valid later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58f2d5-3319-4e1b-bcc1-f8e6d300a82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad54aa-a9c7-4a3f-92f9-467729f23402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f973271-f799-4dad-9839-8c818269e505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65538e8-97c8-444e-b24b-1c525558eada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35e22f-3d87-4a87-84bb-567f7bc15765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fff220-4180-4a52-8865-00cade876aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58436b-b41b-41e7-94b2-33ff0d2586ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e0f5d-6175-4657-92d6-ac68c7943940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382406a-486d-4b1c-8875-a37a8795c929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad51f75-b138-4c7e-a874-abfc06050018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7dbf6-736f-4675-91b0-c7572f41d74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d29066-3318-40cf-a50b-2aed3f5392a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8843e7-ad62-4745-ae71-7f97176a1f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd0afc-0661-4377-ab1c-b7f0b69e144c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29770d5a-91c5-4cbb-9db9-b687cbba4c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f31bf-edaf-48e5-a168-aaa576bbbdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0a8c8-3249-498c-8e1c-121600cee159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34886cc0-a8c4-4801-9940-dbba8c1d9b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318f60f-a8fa-488d-b880-72ceee5d1345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a51eef-551d-464f-9788-6a7ba9332fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ca813-5f23-4e60-9abf-60d93a074aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7eadcb-ac2d-4217-be12-df4f020ab14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec069c-9aa8-4d2e-8abc-5d4a809a2239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8868b20-6a65-4ba2-a469-e1ab3d97f6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674222c8-af86-47a5-abae-ebecb9df6c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4880eb0-9d13-43d8-b4d4-bf4ce491b570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d458b18-21a2-42b2-ade3-d8d6af6204cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a1ebf-5c98-4e95-a1ea-34639a94c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c46c3d-c072-44c2-95b2-b5197bd7c841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
